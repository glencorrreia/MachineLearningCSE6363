{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c2653fd",
   "metadata": {},
   "source": [
    "# Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0114468d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec729205",
   "metadata": {},
   "source": [
    "# activation_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6f2d67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer import Layer\n",
    "\n",
    "# inherit from base class Layer\n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    # returns the activated input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
    "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76c873b",
   "metadata": {},
   "source": [
    "# fc_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5ecce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer import Layer\n",
    "import numpy as np\n",
    "\n",
    "# inherit from base class Layer\n",
    "class FCLayer(Layer):\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of output neurons\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        # dBias = output_error\n",
    "\n",
    "        # update parameters\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1823542d",
   "metadata": {},
   "source": [
    "# activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15d276dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# activation function and its derivative\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d59a63",
   "metadata": {},
   "source": [
    "# conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df0b2c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer import Layer\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "\n",
    "## Math behind this layer can found at : \n",
    "## https://medium.com/@2017csm1006/forward-and-backpropagation-in-convolutional-neural-network-4dfa96d7b37e\n",
    "\n",
    "# inherit from base class Layer\n",
    "# This convolutional layer is always with stride 1\n",
    "class ConvLayer(Layer):\n",
    "    # input_shape = (i,j,d)\n",
    "    # kernel_shape = (m,n)\n",
    "    # layer_depth = output_depth\n",
    "    def __init__(self, input_shape, kernel_shape, layer_depth):\n",
    "        self.input_shape = input_shape\n",
    "        self.input_depth = input_shape[2]\n",
    "        self.kernel_shape = kernel_shape\n",
    "        self.layer_depth = layer_depth\n",
    "        self.output_shape = (input_shape[0]-kernel_shape[0]+1, input_shape[1]-kernel_shape[1]+1, layer_depth)\n",
    "        self.weights = np.random.rand(kernel_shape[0], kernel_shape[1], self.input_depth, layer_depth) - 0.5\n",
    "        self.bias = np.random.rand(layer_depth) - 0.5\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward_propagation(self, input):\n",
    "        self.input = input\n",
    "        self.output = np.zeros(self.output_shape)\n",
    "\n",
    "        for k in range(self.layer_depth):\n",
    "            for d in range(self.input_depth):\n",
    "                self.output[:,:,k] += signal.correlate2d(self.input[:,:,d], self.weights[:,:,d,k], 'valid') + self.bias[k]\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        in_error = np.zeros(self.input_shape)\n",
    "        dWeights = np.zeros((self.kernel_shape[0], self.kernel_shape[1], self.input_depth, self.layer_depth))\n",
    "        dBias = np.zeros(self.layer_depth)\n",
    "\n",
    "        for k in range(self.layer_depth):\n",
    "            for d in range(self.input_depth):\n",
    "                in_error[:,:,d] += signal.convolve2d(output_error[:,:,k], self.weights[:,:,d,k], 'full')\n",
    "                dWeights[:,:,d,k] = signal.correlate2d(self.input[:,:,d], output_error[:,:,k], 'valid')\n",
    "            dBias[k] = self.layer_depth * np.sum(output_error[:,:,k])\n",
    "\n",
    "        self.weights -= learning_rate*dWeights\n",
    "        self.bias -= learning_rate*dBias\n",
    "        return in_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e53677",
   "metadata": {},
   "source": [
    "# flatten_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b95521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer import Layer\n",
    "\n",
    "# inherit from base class Layer\n",
    "class FlattenLayer(Layer):\n",
    "    # returns the flattened input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = input_data.flatten().reshape((1,-1))\n",
    "        return self.output\n",
    "\n",
    "    # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
    "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return output_error.reshape(self.input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcf9702",
   "metadata": {},
   "source": [
    "# losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15ee14d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# loss function and its derivative\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true-y_pred, 2))\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2794753",
   "metadata": {},
   "source": [
    "# network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fd17029",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # set loss to use\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "    # predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        # sample dimension first\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        # run network over all samples\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "    # train the network\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        # sample dimension first\n",
    "        samples = len(x_train)\n",
    "\n",
    "        # training loop\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                # forward propagation\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "                err += self.loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "\n",
    "            # calculate average error on all samples\n",
    "            err /= samples\n",
    "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dabc4d",
   "metadata": {},
   "source": [
    "# example_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1be87443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/1000   error=0.458748\n",
      "epoch 2/1000   error=0.080184\n",
      "epoch 3/1000   error=0.071757\n",
      "epoch 4/1000   error=0.070279\n",
      "epoch 5/1000   error=0.069100\n",
      "epoch 6/1000   error=0.068003\n",
      "epoch 7/1000   error=0.066970\n",
      "epoch 8/1000   error=0.065993\n",
      "epoch 9/1000   error=0.065063\n",
      "epoch 10/1000   error=0.064175\n",
      "epoch 11/1000   error=0.063323\n",
      "epoch 12/1000   error=0.062505\n",
      "epoch 13/1000   error=0.061714\n",
      "epoch 14/1000   error=0.060949\n",
      "epoch 15/1000   error=0.060207\n",
      "epoch 16/1000   error=0.059485\n",
      "epoch 17/1000   error=0.058781\n",
      "epoch 18/1000   error=0.058094\n",
      "epoch 19/1000   error=0.057423\n",
      "epoch 20/1000   error=0.056765\n",
      "epoch 21/1000   error=0.056121\n",
      "epoch 22/1000   error=0.055489\n",
      "epoch 23/1000   error=0.054870\n",
      "epoch 24/1000   error=0.054262\n",
      "epoch 25/1000   error=0.053665\n",
      "epoch 26/1000   error=0.053080\n",
      "epoch 27/1000   error=0.052505\n",
      "epoch 28/1000   error=0.051942\n",
      "epoch 29/1000   error=0.051389\n",
      "epoch 30/1000   error=0.050848\n",
      "epoch 31/1000   error=0.050317\n",
      "epoch 32/1000   error=0.049798\n",
      "epoch 33/1000   error=0.049290\n",
      "epoch 34/1000   error=0.048793\n",
      "epoch 35/1000   error=0.048306\n",
      "epoch 36/1000   error=0.047831\n",
      "epoch 37/1000   error=0.047366\n",
      "epoch 38/1000   error=0.046913\n",
      "epoch 39/1000   error=0.046469\n",
      "epoch 40/1000   error=0.046036\n",
      "epoch 41/1000   error=0.045612\n",
      "epoch 42/1000   error=0.045198\n",
      "epoch 43/1000   error=0.044794\n",
      "epoch 44/1000   error=0.044398\n",
      "epoch 45/1000   error=0.044010\n",
      "epoch 46/1000   error=0.043631\n",
      "epoch 47/1000   error=0.043258\n",
      "epoch 48/1000   error=0.042893\n",
      "epoch 49/1000   error=0.042535\n",
      "epoch 50/1000   error=0.042182\n",
      "epoch 51/1000   error=0.041836\n",
      "epoch 52/1000   error=0.041494\n",
      "epoch 53/1000   error=0.041158\n",
      "epoch 54/1000   error=0.040826\n",
      "epoch 55/1000   error=0.040498\n",
      "epoch 56/1000   error=0.040175\n",
      "epoch 57/1000   error=0.039855\n",
      "epoch 58/1000   error=0.039538\n",
      "epoch 59/1000   error=0.039224\n",
      "epoch 60/1000   error=0.038913\n",
      "epoch 61/1000   error=0.038605\n",
      "epoch 62/1000   error=0.038299\n",
      "epoch 63/1000   error=0.037996\n",
      "epoch 64/1000   error=0.037694\n",
      "epoch 65/1000   error=0.037395\n",
      "epoch 66/1000   error=0.037098\n",
      "epoch 67/1000   error=0.036803\n",
      "epoch 68/1000   error=0.036511\n",
      "epoch 69/1000   error=0.036220\n",
      "epoch 70/1000   error=0.035931\n",
      "epoch 71/1000   error=0.035644\n",
      "epoch 72/1000   error=0.035358\n",
      "epoch 73/1000   error=0.035075\n",
      "epoch 74/1000   error=0.034794\n",
      "epoch 75/1000   error=0.034516\n",
      "epoch 76/1000   error=0.034239\n",
      "epoch 77/1000   error=0.033965\n",
      "epoch 78/1000   error=0.033693\n",
      "epoch 79/1000   error=0.033423\n",
      "epoch 80/1000   error=0.033156\n",
      "epoch 81/1000   error=0.032892\n",
      "epoch 82/1000   error=0.032630\n",
      "epoch 83/1000   error=0.032371\n",
      "epoch 84/1000   error=0.032114\n",
      "epoch 85/1000   error=0.031861\n",
      "epoch 86/1000   error=0.031610\n",
      "epoch 87/1000   error=0.031363\n",
      "epoch 88/1000   error=0.031118\n",
      "epoch 89/1000   error=0.030877\n",
      "epoch 90/1000   error=0.030639\n",
      "epoch 91/1000   error=0.030405\n",
      "epoch 92/1000   error=0.030173\n",
      "epoch 93/1000   error=0.029945\n",
      "epoch 94/1000   error=0.029720\n",
      "epoch 95/1000   error=0.029499\n",
      "epoch 96/1000   error=0.029281\n",
      "epoch 97/1000   error=0.029066\n",
      "epoch 98/1000   error=0.028855\n",
      "epoch 99/1000   error=0.028648\n",
      "epoch 100/1000   error=0.028443\n",
      "epoch 101/1000   error=0.028242\n",
      "epoch 102/1000   error=0.028045\n",
      "epoch 103/1000   error=0.027850\n",
      "epoch 104/1000   error=0.027659\n",
      "epoch 105/1000   error=0.027472\n",
      "epoch 106/1000   error=0.027287\n",
      "epoch 107/1000   error=0.027106\n",
      "epoch 108/1000   error=0.026928\n",
      "epoch 109/1000   error=0.026753\n",
      "epoch 110/1000   error=0.026581\n",
      "epoch 111/1000   error=0.026412\n",
      "epoch 112/1000   error=0.026245\n",
      "epoch 113/1000   error=0.026082\n",
      "epoch 114/1000   error=0.025922\n",
      "epoch 115/1000   error=0.025764\n",
      "epoch 116/1000   error=0.025609\n",
      "epoch 117/1000   error=0.025456\n",
      "epoch 118/1000   error=0.025306\n",
      "epoch 119/1000   error=0.025159\n",
      "epoch 120/1000   error=0.025013\n",
      "epoch 121/1000   error=0.024871\n",
      "epoch 122/1000   error=0.024730\n",
      "epoch 123/1000   error=0.024592\n",
      "epoch 124/1000   error=0.024455\n",
      "epoch 125/1000   error=0.024321\n",
      "epoch 126/1000   error=0.024189\n",
      "epoch 127/1000   error=0.024059\n",
      "epoch 128/1000   error=0.023930\n",
      "epoch 129/1000   error=0.023803\n",
      "epoch 130/1000   error=0.023679\n",
      "epoch 131/1000   error=0.023555\n",
      "epoch 132/1000   error=0.023434\n",
      "epoch 133/1000   error=0.023314\n",
      "epoch 134/1000   error=0.023195\n",
      "epoch 135/1000   error=0.023078\n",
      "epoch 136/1000   error=0.022963\n",
      "epoch 137/1000   error=0.022848\n",
      "epoch 138/1000   error=0.022735\n",
      "epoch 139/1000   error=0.022624\n",
      "epoch 140/1000   error=0.022513\n",
      "epoch 141/1000   error=0.022404\n",
      "epoch 142/1000   error=0.022295\n",
      "epoch 143/1000   error=0.022188\n",
      "epoch 144/1000   error=0.022082\n",
      "epoch 145/1000   error=0.021977\n",
      "epoch 146/1000   error=0.021872\n",
      "epoch 147/1000   error=0.021769\n",
      "epoch 148/1000   error=0.021666\n",
      "epoch 149/1000   error=0.021565\n",
      "epoch 150/1000   error=0.021464\n",
      "epoch 151/1000   error=0.021363\n",
      "epoch 152/1000   error=0.021264\n",
      "epoch 153/1000   error=0.021165\n",
      "epoch 154/1000   error=0.021067\n",
      "epoch 155/1000   error=0.020970\n",
      "epoch 156/1000   error=0.020873\n",
      "epoch 157/1000   error=0.020777\n",
      "epoch 158/1000   error=0.020681\n",
      "epoch 159/1000   error=0.020586\n",
      "epoch 160/1000   error=0.020491\n",
      "epoch 161/1000   error=0.020397\n",
      "epoch 162/1000   error=0.020303\n",
      "epoch 163/1000   error=0.020210\n",
      "epoch 164/1000   error=0.020117\n",
      "epoch 165/1000   error=0.020025\n",
      "epoch 166/1000   error=0.019933\n",
      "epoch 167/1000   error=0.019841\n",
      "epoch 168/1000   error=0.019750\n",
      "epoch 169/1000   error=0.019659\n",
      "epoch 170/1000   error=0.019568\n",
      "epoch 171/1000   error=0.019478\n",
      "epoch 172/1000   error=0.019388\n",
      "epoch 173/1000   error=0.019298\n",
      "epoch 174/1000   error=0.019209\n",
      "epoch 175/1000   error=0.019120\n",
      "epoch 176/1000   error=0.019031\n",
      "epoch 177/1000   error=0.018943\n",
      "epoch 178/1000   error=0.018854\n",
      "epoch 179/1000   error=0.018766\n",
      "epoch 180/1000   error=0.018679\n",
      "epoch 181/1000   error=0.018591\n",
      "epoch 182/1000   error=0.018504\n",
      "epoch 183/1000   error=0.018416\n",
      "epoch 184/1000   error=0.018330\n",
      "epoch 185/1000   error=0.018243\n",
      "epoch 186/1000   error=0.018156\n",
      "epoch 187/1000   error=0.018070\n",
      "epoch 188/1000   error=0.017984\n",
      "epoch 189/1000   error=0.017898\n",
      "epoch 190/1000   error=0.017812\n",
      "epoch 191/1000   error=0.017727\n",
      "epoch 192/1000   error=0.017642\n",
      "epoch 193/1000   error=0.017557\n",
      "epoch 194/1000   error=0.017472\n",
      "epoch 195/1000   error=0.017387\n",
      "epoch 196/1000   error=0.017302\n",
      "epoch 197/1000   error=0.017218\n",
      "epoch 198/1000   error=0.017134\n",
      "epoch 199/1000   error=0.017050\n",
      "epoch 200/1000   error=0.016966\n",
      "epoch 201/1000   error=0.016883\n",
      "epoch 202/1000   error=0.016799\n",
      "epoch 203/1000   error=0.016716\n",
      "epoch 204/1000   error=0.016633\n",
      "epoch 205/1000   error=0.016550\n",
      "epoch 206/1000   error=0.016468\n",
      "epoch 207/1000   error=0.016385\n",
      "epoch 208/1000   error=0.016303\n",
      "epoch 209/1000   error=0.016221\n",
      "epoch 210/1000   error=0.016139\n",
      "epoch 211/1000   error=0.016058\n",
      "epoch 212/1000   error=0.015977\n",
      "epoch 213/1000   error=0.015895\n",
      "epoch 214/1000   error=0.015814\n",
      "epoch 215/1000   error=0.015734\n",
      "epoch 216/1000   error=0.015653\n",
      "epoch 217/1000   error=0.015573\n",
      "epoch 218/1000   error=0.015493\n",
      "epoch 219/1000   error=0.015413\n",
      "epoch 220/1000   error=0.015334\n",
      "epoch 221/1000   error=0.015254\n",
      "epoch 222/1000   error=0.015175\n",
      "epoch 223/1000   error=0.015096\n",
      "epoch 224/1000   error=0.015017\n",
      "epoch 225/1000   error=0.014939\n",
      "epoch 226/1000   error=0.014861\n",
      "epoch 227/1000   error=0.014783\n",
      "epoch 228/1000   error=0.014705\n",
      "epoch 229/1000   error=0.014628\n",
      "epoch 230/1000   error=0.014551\n",
      "epoch 231/1000   error=0.014474\n",
      "epoch 232/1000   error=0.014397\n",
      "epoch 233/1000   error=0.014320\n",
      "epoch 234/1000   error=0.014244\n",
      "epoch 235/1000   error=0.014168\n",
      "epoch 236/1000   error=0.014093\n",
      "epoch 237/1000   error=0.014017\n",
      "epoch 238/1000   error=0.013942\n",
      "epoch 239/1000   error=0.013867\n",
      "epoch 240/1000   error=0.013793\n",
      "epoch 241/1000   error=0.013718\n",
      "epoch 242/1000   error=0.013644\n",
      "epoch 243/1000   error=0.013570\n",
      "epoch 244/1000   error=0.013497\n",
      "epoch 245/1000   error=0.013424\n",
      "epoch 246/1000   error=0.013351\n",
      "epoch 247/1000   error=0.013278\n",
      "epoch 248/1000   error=0.013205\n",
      "epoch 249/1000   error=0.013133\n",
      "epoch 250/1000   error=0.013061\n",
      "epoch 251/1000   error=0.012990\n",
      "epoch 252/1000   error=0.012918\n",
      "epoch 253/1000   error=0.012847\n",
      "epoch 254/1000   error=0.012777\n",
      "epoch 255/1000   error=0.012706\n",
      "epoch 256/1000   error=0.012636\n",
      "epoch 257/1000   error=0.012566\n",
      "epoch 258/1000   error=0.012497\n",
      "epoch 259/1000   error=0.012427\n",
      "epoch 260/1000   error=0.012358\n",
      "epoch 261/1000   error=0.012289\n",
      "epoch 262/1000   error=0.012221\n",
      "epoch 263/1000   error=0.012153\n",
      "epoch 264/1000   error=0.012085\n",
      "epoch 265/1000   error=0.012017\n",
      "epoch 266/1000   error=0.011950\n",
      "epoch 267/1000   error=0.011883\n",
      "epoch 268/1000   error=0.011816\n",
      "epoch 269/1000   error=0.011750\n",
      "epoch 270/1000   error=0.011684\n",
      "epoch 271/1000   error=0.011618\n",
      "epoch 272/1000   error=0.011553\n",
      "epoch 273/1000   error=0.011487\n",
      "epoch 274/1000   error=0.011422\n",
      "epoch 275/1000   error=0.011358\n",
      "epoch 276/1000   error=0.011294\n",
      "epoch 277/1000   error=0.011230\n",
      "epoch 278/1000   error=0.011166\n",
      "epoch 279/1000   error=0.011102\n",
      "epoch 280/1000   error=0.011039\n",
      "epoch 281/1000   error=0.010976\n",
      "epoch 282/1000   error=0.010914\n",
      "epoch 283/1000   error=0.010852\n",
      "epoch 284/1000   error=0.010790\n",
      "epoch 285/1000   error=0.010728\n",
      "epoch 286/1000   error=0.010667\n",
      "epoch 287/1000   error=0.010606\n",
      "epoch 288/1000   error=0.010545\n",
      "epoch 289/1000   error=0.010485\n",
      "epoch 290/1000   error=0.010425\n",
      "epoch 291/1000   error=0.010365\n",
      "epoch 292/1000   error=0.010306\n",
      "epoch 293/1000   error=0.010246\n",
      "epoch 294/1000   error=0.010188\n",
      "epoch 295/1000   error=0.010129\n",
      "epoch 296/1000   error=0.010071\n",
      "epoch 297/1000   error=0.010013\n",
      "epoch 298/1000   error=0.009955\n",
      "epoch 299/1000   error=0.009898\n",
      "epoch 300/1000   error=0.009841\n",
      "epoch 301/1000   error=0.009784\n",
      "epoch 302/1000   error=0.009728\n",
      "epoch 303/1000   error=0.009672\n",
      "epoch 304/1000   error=0.009616\n",
      "epoch 305/1000   error=0.009560\n",
      "epoch 306/1000   error=0.009505\n",
      "epoch 307/1000   error=0.009450\n",
      "epoch 308/1000   error=0.009396\n",
      "epoch 309/1000   error=0.009341\n",
      "epoch 310/1000   error=0.009287\n",
      "epoch 311/1000   error=0.009234\n",
      "epoch 312/1000   error=0.009180\n",
      "epoch 313/1000   error=0.009127\n",
      "epoch 314/1000   error=0.009074\n",
      "epoch 315/1000   error=0.009022\n",
      "epoch 316/1000   error=0.008970\n",
      "epoch 317/1000   error=0.008918\n",
      "epoch 318/1000   error=0.008866\n",
      "epoch 319/1000   error=0.008815\n",
      "epoch 320/1000   error=0.008764\n",
      "epoch 321/1000   error=0.008713\n",
      "epoch 322/1000   error=0.008663\n",
      "epoch 323/1000   error=0.008613\n",
      "epoch 324/1000   error=0.008563\n",
      "epoch 325/1000   error=0.008514\n",
      "epoch 326/1000   error=0.008465\n",
      "epoch 327/1000   error=0.008416\n",
      "epoch 328/1000   error=0.008367\n",
      "epoch 329/1000   error=0.008319\n",
      "epoch 330/1000   error=0.008271\n",
      "epoch 331/1000   error=0.008223\n",
      "epoch 332/1000   error=0.008176\n",
      "epoch 333/1000   error=0.008128\n",
      "epoch 334/1000   error=0.008082\n",
      "epoch 335/1000   error=0.008035\n",
      "epoch 336/1000   error=0.007989\n",
      "epoch 337/1000   error=0.007943\n",
      "epoch 338/1000   error=0.007897\n",
      "epoch 339/1000   error=0.007852\n",
      "epoch 340/1000   error=0.007807\n",
      "epoch 341/1000   error=0.007762\n",
      "epoch 342/1000   error=0.007717\n",
      "epoch 343/1000   error=0.007673\n",
      "epoch 344/1000   error=0.007629\n",
      "epoch 345/1000   error=0.007585\n",
      "epoch 346/1000   error=0.007542\n",
      "epoch 347/1000   error=0.007498\n",
      "epoch 348/1000   error=0.007456\n",
      "epoch 349/1000   error=0.007413\n",
      "epoch 350/1000   error=0.007371\n",
      "epoch 351/1000   error=0.007328\n",
      "epoch 352/1000   error=0.007287\n",
      "epoch 353/1000   error=0.007245\n",
      "epoch 354/1000   error=0.007204\n",
      "epoch 355/1000   error=0.007163\n",
      "epoch 356/1000   error=0.007122\n",
      "epoch 357/1000   error=0.007082\n",
      "epoch 358/1000   error=0.007041\n",
      "epoch 359/1000   error=0.007001\n",
      "epoch 360/1000   error=0.006962\n",
      "epoch 361/1000   error=0.006922\n",
      "epoch 362/1000   error=0.006883\n",
      "epoch 363/1000   error=0.006844\n",
      "epoch 364/1000   error=0.006805\n",
      "epoch 365/1000   error=0.006767\n",
      "epoch 366/1000   error=0.006729\n",
      "epoch 367/1000   error=0.006691\n",
      "epoch 368/1000   error=0.006653\n",
      "epoch 369/1000   error=0.006616\n",
      "epoch 370/1000   error=0.006579\n",
      "epoch 371/1000   error=0.006542\n",
      "epoch 372/1000   error=0.006505\n",
      "epoch 373/1000   error=0.006469\n",
      "epoch 374/1000   error=0.006432\n",
      "epoch 375/1000   error=0.006397\n",
      "epoch 376/1000   error=0.006361\n",
      "epoch 377/1000   error=0.006325\n",
      "epoch 378/1000   error=0.006290\n",
      "epoch 379/1000   error=0.006255\n",
      "epoch 380/1000   error=0.006220\n",
      "epoch 381/1000   error=0.006186\n",
      "epoch 382/1000   error=0.006152\n",
      "epoch 383/1000   error=0.006118\n",
      "epoch 384/1000   error=0.006084\n",
      "epoch 385/1000   error=0.006050\n",
      "epoch 386/1000   error=0.006017\n",
      "epoch 387/1000   error=0.005984\n",
      "epoch 388/1000   error=0.005951\n",
      "epoch 389/1000   error=0.005918\n",
      "epoch 390/1000   error=0.005886\n",
      "epoch 391/1000   error=0.005853\n",
      "epoch 392/1000   error=0.005821\n",
      "epoch 393/1000   error=0.005789\n",
      "epoch 394/1000   error=0.005758\n",
      "epoch 395/1000   error=0.005726\n",
      "epoch 396/1000   error=0.005695\n",
      "epoch 397/1000   error=0.005664\n",
      "epoch 398/1000   error=0.005634\n",
      "epoch 399/1000   error=0.005603\n",
      "epoch 400/1000   error=0.005573\n",
      "epoch 401/1000   error=0.005543\n",
      "epoch 402/1000   error=0.005513\n",
      "epoch 403/1000   error=0.005483\n",
      "epoch 404/1000   error=0.005454\n",
      "epoch 405/1000   error=0.005424\n",
      "epoch 406/1000   error=0.005395\n",
      "epoch 407/1000   error=0.005366\n",
      "epoch 408/1000   error=0.005338\n",
      "epoch 409/1000   error=0.005309\n",
      "epoch 410/1000   error=0.005281\n",
      "epoch 411/1000   error=0.005253\n",
      "epoch 412/1000   error=0.005225\n",
      "epoch 413/1000   error=0.005197\n",
      "epoch 414/1000   error=0.005170\n",
      "epoch 415/1000   error=0.005142\n",
      "epoch 416/1000   error=0.005115\n",
      "epoch 417/1000   error=0.005088\n",
      "epoch 418/1000   error=0.005061\n",
      "epoch 419/1000   error=0.005035\n",
      "epoch 420/1000   error=0.005008\n",
      "epoch 421/1000   error=0.004982\n",
      "epoch 422/1000   error=0.004956\n",
      "epoch 423/1000   error=0.004930\n",
      "epoch 424/1000   error=0.004905\n",
      "epoch 425/1000   error=0.004879\n",
      "epoch 426/1000   error=0.004854\n",
      "epoch 427/1000   error=0.004829\n",
      "epoch 428/1000   error=0.004804\n",
      "epoch 429/1000   error=0.004779\n",
      "epoch 430/1000   error=0.004754\n",
      "epoch 431/1000   error=0.004730\n",
      "epoch 432/1000   error=0.004705\n",
      "epoch 433/1000   error=0.004681\n",
      "epoch 434/1000   error=0.004657\n",
      "epoch 435/1000   error=0.004634\n",
      "epoch 436/1000   error=0.004610\n",
      "epoch 437/1000   error=0.004586\n",
      "epoch 438/1000   error=0.004563\n",
      "epoch 439/1000   error=0.004540\n",
      "epoch 440/1000   error=0.004517\n",
      "epoch 441/1000   error=0.004494\n",
      "epoch 442/1000   error=0.004471\n",
      "epoch 443/1000   error=0.004449\n",
      "epoch 444/1000   error=0.004427\n",
      "epoch 445/1000   error=0.004404\n",
      "epoch 446/1000   error=0.004382\n",
      "epoch 447/1000   error=0.004360\n",
      "epoch 448/1000   error=0.004339\n",
      "epoch 449/1000   error=0.004317\n",
      "epoch 450/1000   error=0.004296\n",
      "epoch 451/1000   error=0.004274\n",
      "epoch 452/1000   error=0.004253\n",
      "epoch 453/1000   error=0.004232\n",
      "epoch 454/1000   error=0.004211\n",
      "epoch 455/1000   error=0.004190\n",
      "epoch 456/1000   error=0.004170\n",
      "epoch 457/1000   error=0.004149\n",
      "epoch 458/1000   error=0.004129\n",
      "epoch 459/1000   error=0.004109\n",
      "epoch 460/1000   error=0.004089\n",
      "epoch 461/1000   error=0.004069\n",
      "epoch 462/1000   error=0.004049\n",
      "epoch 463/1000   error=0.004029\n",
      "epoch 464/1000   error=0.004010\n",
      "epoch 465/1000   error=0.003990\n",
      "epoch 466/1000   error=0.003971\n",
      "epoch 467/1000   error=0.003952\n",
      "epoch 468/1000   error=0.003933\n",
      "epoch 469/1000   error=0.003914\n",
      "epoch 470/1000   error=0.003895\n",
      "epoch 471/1000   error=0.003877\n",
      "epoch 472/1000   error=0.003858\n",
      "epoch 473/1000   error=0.003840\n",
      "epoch 474/1000   error=0.003822\n",
      "epoch 475/1000   error=0.003804\n",
      "epoch 476/1000   error=0.003786\n",
      "epoch 477/1000   error=0.003768\n",
      "epoch 478/1000   error=0.003750\n",
      "epoch 479/1000   error=0.003732\n",
      "epoch 480/1000   error=0.003715\n",
      "epoch 481/1000   error=0.003697\n",
      "epoch 482/1000   error=0.003680\n",
      "epoch 483/1000   error=0.003663\n",
      "epoch 484/1000   error=0.003646\n",
      "epoch 485/1000   error=0.003629\n",
      "epoch 486/1000   error=0.003612\n",
      "epoch 487/1000   error=0.003595\n",
      "epoch 488/1000   error=0.003579\n",
      "epoch 489/1000   error=0.003562\n",
      "epoch 490/1000   error=0.003546\n",
      "epoch 491/1000   error=0.003529\n",
      "epoch 492/1000   error=0.003513\n",
      "epoch 493/1000   error=0.003497\n",
      "epoch 494/1000   error=0.003481\n",
      "epoch 495/1000   error=0.003465\n",
      "epoch 496/1000   error=0.003449\n",
      "epoch 497/1000   error=0.003434\n",
      "epoch 498/1000   error=0.003418\n",
      "epoch 499/1000   error=0.003402\n",
      "epoch 500/1000   error=0.003387\n",
      "epoch 501/1000   error=0.003372\n",
      "epoch 502/1000   error=0.003357\n",
      "epoch 503/1000   error=0.003341\n",
      "epoch 504/1000   error=0.003326\n",
      "epoch 505/1000   error=0.003311\n",
      "epoch 506/1000   error=0.003297\n",
      "epoch 507/1000   error=0.003282\n",
      "epoch 508/1000   error=0.003267\n",
      "epoch 509/1000   error=0.003253\n",
      "epoch 510/1000   error=0.003238\n",
      "epoch 511/1000   error=0.003224\n",
      "epoch 512/1000   error=0.003210\n",
      "epoch 513/1000   error=0.003195\n",
      "epoch 514/1000   error=0.003181\n",
      "epoch 515/1000   error=0.003167\n",
      "epoch 516/1000   error=0.003153\n",
      "epoch 517/1000   error=0.003139\n",
      "epoch 518/1000   error=0.003126\n",
      "epoch 519/1000   error=0.003112\n",
      "epoch 520/1000   error=0.003098\n",
      "epoch 521/1000   error=0.003085\n",
      "epoch 522/1000   error=0.003071\n",
      "epoch 523/1000   error=0.003058\n",
      "epoch 524/1000   error=0.003045\n",
      "epoch 525/1000   error=0.003032\n",
      "epoch 526/1000   error=0.003019\n",
      "epoch 527/1000   error=0.003006\n",
      "epoch 528/1000   error=0.002993\n",
      "epoch 529/1000   error=0.002980\n",
      "epoch 530/1000   error=0.002967\n",
      "epoch 531/1000   error=0.002954\n",
      "epoch 532/1000   error=0.002942\n",
      "epoch 533/1000   error=0.002929\n",
      "epoch 534/1000   error=0.002916\n",
      "epoch 535/1000   error=0.002904\n",
      "epoch 536/1000   error=0.002892\n",
      "epoch 537/1000   error=0.002879\n",
      "epoch 538/1000   error=0.002867\n",
      "epoch 539/1000   error=0.002855\n",
      "epoch 540/1000   error=0.002843\n",
      "epoch 541/1000   error=0.002831\n",
      "epoch 542/1000   error=0.002819\n",
      "epoch 543/1000   error=0.002807\n",
      "epoch 544/1000   error=0.002795\n",
      "epoch 545/1000   error=0.002784\n",
      "epoch 546/1000   error=0.002772\n",
      "epoch 547/1000   error=0.002760\n",
      "epoch 548/1000   error=0.002749\n",
      "epoch 549/1000   error=0.002737\n",
      "epoch 550/1000   error=0.002726\n",
      "epoch 551/1000   error=0.002715\n",
      "epoch 552/1000   error=0.002703\n",
      "epoch 553/1000   error=0.002692\n",
      "epoch 554/1000   error=0.002681\n",
      "epoch 555/1000   error=0.002670\n",
      "epoch 556/1000   error=0.002659\n",
      "epoch 557/1000   error=0.002648\n",
      "epoch 558/1000   error=0.002637\n",
      "epoch 559/1000   error=0.002626\n",
      "epoch 560/1000   error=0.002615\n",
      "epoch 561/1000   error=0.002605\n",
      "epoch 562/1000   error=0.002594\n",
      "epoch 563/1000   error=0.002584\n",
      "epoch 564/1000   error=0.002573\n",
      "epoch 565/1000   error=0.002563\n",
      "epoch 566/1000   error=0.002552\n",
      "epoch 567/1000   error=0.002542\n",
      "epoch 568/1000   error=0.002531\n",
      "epoch 569/1000   error=0.002521\n",
      "epoch 570/1000   error=0.002511\n",
      "epoch 571/1000   error=0.002501\n",
      "epoch 572/1000   error=0.002491\n",
      "epoch 573/1000   error=0.002481\n",
      "epoch 574/1000   error=0.002471\n",
      "epoch 575/1000   error=0.002461\n",
      "epoch 576/1000   error=0.002451\n",
      "epoch 577/1000   error=0.002441\n",
      "epoch 578/1000   error=0.002431\n",
      "epoch 579/1000   error=0.002422\n",
      "epoch 580/1000   error=0.002412\n",
      "epoch 581/1000   error=0.002402\n",
      "epoch 582/1000   error=0.002393\n",
      "epoch 583/1000   error=0.002383\n",
      "epoch 584/1000   error=0.002374\n",
      "epoch 585/1000   error=0.002364\n",
      "epoch 586/1000   error=0.002355\n",
      "epoch 587/1000   error=0.002346\n",
      "epoch 588/1000   error=0.002336\n",
      "epoch 589/1000   error=0.002327\n",
      "epoch 590/1000   error=0.002318\n",
      "epoch 591/1000   error=0.002309\n",
      "epoch 592/1000   error=0.002300\n",
      "epoch 593/1000   error=0.002291\n",
      "epoch 594/1000   error=0.002282\n",
      "epoch 595/1000   error=0.002273\n",
      "epoch 596/1000   error=0.002264\n",
      "epoch 597/1000   error=0.002255\n",
      "epoch 598/1000   error=0.002246\n",
      "epoch 599/1000   error=0.002238\n",
      "epoch 600/1000   error=0.002229\n",
      "epoch 601/1000   error=0.002220\n",
      "epoch 602/1000   error=0.002212\n",
      "epoch 603/1000   error=0.002203\n",
      "epoch 604/1000   error=0.002195\n",
      "epoch 605/1000   error=0.002186\n",
      "epoch 606/1000   error=0.002178\n",
      "epoch 607/1000   error=0.002169\n",
      "epoch 608/1000   error=0.002161\n",
      "epoch 609/1000   error=0.002153\n",
      "epoch 610/1000   error=0.002144\n",
      "epoch 611/1000   error=0.002136\n",
      "epoch 612/1000   error=0.002128\n",
      "epoch 613/1000   error=0.002120\n",
      "epoch 614/1000   error=0.002112\n",
      "epoch 615/1000   error=0.002104\n",
      "epoch 616/1000   error=0.002096\n",
      "epoch 617/1000   error=0.002088\n",
      "epoch 618/1000   error=0.002080\n",
      "epoch 619/1000   error=0.002072\n",
      "epoch 620/1000   error=0.002064\n",
      "epoch 621/1000   error=0.002056\n",
      "epoch 622/1000   error=0.002048\n",
      "epoch 623/1000   error=0.002041\n",
      "epoch 624/1000   error=0.002033\n",
      "epoch 625/1000   error=0.002025\n",
      "epoch 626/1000   error=0.002018\n",
      "epoch 627/1000   error=0.002010\n",
      "epoch 628/1000   error=0.002003\n",
      "epoch 629/1000   error=0.001995\n",
      "epoch 630/1000   error=0.001988\n",
      "epoch 631/1000   error=0.001980\n",
      "epoch 632/1000   error=0.001973\n",
      "epoch 633/1000   error=0.001965\n",
      "epoch 634/1000   error=0.001958\n",
      "epoch 635/1000   error=0.001951\n",
      "epoch 636/1000   error=0.001944\n",
      "epoch 637/1000   error=0.001936\n",
      "epoch 638/1000   error=0.001929\n",
      "epoch 639/1000   error=0.001922\n",
      "epoch 640/1000   error=0.001915\n",
      "epoch 641/1000   error=0.001908\n",
      "epoch 642/1000   error=0.001901\n",
      "epoch 643/1000   error=0.001894\n",
      "epoch 644/1000   error=0.001887\n",
      "epoch 645/1000   error=0.001880\n",
      "epoch 646/1000   error=0.001873\n",
      "epoch 647/1000   error=0.001866\n",
      "epoch 648/1000   error=0.001859\n",
      "epoch 649/1000   error=0.001853\n",
      "epoch 650/1000   error=0.001846\n",
      "epoch 651/1000   error=0.001839\n",
      "epoch 652/1000   error=0.001832\n",
      "epoch 653/1000   error=0.001826\n",
      "epoch 654/1000   error=0.001819\n",
      "epoch 655/1000   error=0.001812\n",
      "epoch 656/1000   error=0.001806\n",
      "epoch 657/1000   error=0.001799\n",
      "epoch 658/1000   error=0.001793\n",
      "epoch 659/1000   error=0.001786\n",
      "epoch 660/1000   error=0.001780\n",
      "epoch 661/1000   error=0.001774\n",
      "epoch 662/1000   error=0.001767\n",
      "epoch 663/1000   error=0.001761\n",
      "epoch 664/1000   error=0.001755\n",
      "epoch 665/1000   error=0.001748\n",
      "epoch 666/1000   error=0.001742\n",
      "epoch 667/1000   error=0.001736\n",
      "epoch 668/1000   error=0.001730\n",
      "epoch 669/1000   error=0.001723\n",
      "epoch 670/1000   error=0.001717\n",
      "epoch 671/1000   error=0.001711\n",
      "epoch 672/1000   error=0.001705\n",
      "epoch 673/1000   error=0.001699\n",
      "epoch 674/1000   error=0.001693\n",
      "epoch 675/1000   error=0.001687\n",
      "epoch 676/1000   error=0.001681\n",
      "epoch 677/1000   error=0.001675\n",
      "epoch 678/1000   error=0.001669\n",
      "epoch 679/1000   error=0.001664\n",
      "epoch 680/1000   error=0.001658\n",
      "epoch 681/1000   error=0.001652\n",
      "epoch 682/1000   error=0.001646\n",
      "epoch 683/1000   error=0.001640\n",
      "epoch 684/1000   error=0.001635\n",
      "epoch 685/1000   error=0.001629\n",
      "epoch 686/1000   error=0.001623\n",
      "epoch 687/1000   error=0.001618\n",
      "epoch 688/1000   error=0.001612\n",
      "epoch 689/1000   error=0.001607\n",
      "epoch 690/1000   error=0.001601\n",
      "epoch 691/1000   error=0.001595\n",
      "epoch 692/1000   error=0.001590\n",
      "epoch 693/1000   error=0.001585\n",
      "epoch 694/1000   error=0.001579\n",
      "epoch 695/1000   error=0.001574\n",
      "epoch 696/1000   error=0.001568\n",
      "epoch 697/1000   error=0.001563\n",
      "epoch 698/1000   error=0.001558\n",
      "epoch 699/1000   error=0.001552\n",
      "epoch 700/1000   error=0.001547\n",
      "epoch 701/1000   error=0.001542\n",
      "epoch 702/1000   error=0.001537\n",
      "epoch 703/1000   error=0.001531\n",
      "epoch 704/1000   error=0.001526\n",
      "epoch 705/1000   error=0.001521\n",
      "epoch 706/1000   error=0.001516\n",
      "epoch 707/1000   error=0.001511\n",
      "epoch 708/1000   error=0.001506\n",
      "epoch 709/1000   error=0.001501\n",
      "epoch 710/1000   error=0.001496\n",
      "epoch 711/1000   error=0.001491\n",
      "epoch 712/1000   error=0.001486\n",
      "epoch 713/1000   error=0.001481\n",
      "epoch 714/1000   error=0.001476\n",
      "epoch 715/1000   error=0.001471\n",
      "epoch 716/1000   error=0.001466\n",
      "epoch 717/1000   error=0.001461\n",
      "epoch 718/1000   error=0.001456\n",
      "epoch 719/1000   error=0.001452\n",
      "epoch 720/1000   error=0.001447\n",
      "epoch 721/1000   error=0.001442\n",
      "epoch 722/1000   error=0.001437\n",
      "epoch 723/1000   error=0.001433\n",
      "epoch 724/1000   error=0.001428\n",
      "epoch 725/1000   error=0.001423\n",
      "epoch 726/1000   error=0.001419\n",
      "epoch 727/1000   error=0.001414\n",
      "epoch 728/1000   error=0.001409\n",
      "epoch 729/1000   error=0.001405\n",
      "epoch 730/1000   error=0.001400\n",
      "epoch 731/1000   error=0.001396\n",
      "epoch 732/1000   error=0.001391\n",
      "epoch 733/1000   error=0.001387\n",
      "epoch 734/1000   error=0.001382\n",
      "epoch 735/1000   error=0.001378\n",
      "epoch 736/1000   error=0.001374\n",
      "epoch 737/1000   error=0.001369\n",
      "epoch 738/1000   error=0.001365\n",
      "epoch 739/1000   error=0.001360\n",
      "epoch 740/1000   error=0.001356\n",
      "epoch 741/1000   error=0.001352\n",
      "epoch 742/1000   error=0.001348\n",
      "epoch 743/1000   error=0.001343\n",
      "epoch 744/1000   error=0.001339\n",
      "epoch 745/1000   error=0.001335\n",
      "epoch 746/1000   error=0.001331\n",
      "epoch 747/1000   error=0.001326\n",
      "epoch 748/1000   error=0.001322\n",
      "epoch 749/1000   error=0.001318\n",
      "epoch 750/1000   error=0.001314\n",
      "epoch 751/1000   error=0.001310\n",
      "epoch 752/1000   error=0.001306\n",
      "epoch 753/1000   error=0.001302\n",
      "epoch 754/1000   error=0.001298\n",
      "epoch 755/1000   error=0.001294\n",
      "epoch 756/1000   error=0.001290\n",
      "epoch 757/1000   error=0.001286\n",
      "epoch 758/1000   error=0.001282\n",
      "epoch 759/1000   error=0.001278\n",
      "epoch 760/1000   error=0.001274\n",
      "epoch 761/1000   error=0.001270\n",
      "epoch 762/1000   error=0.001266\n",
      "epoch 763/1000   error=0.001262\n",
      "epoch 764/1000   error=0.001259\n",
      "epoch 765/1000   error=0.001255\n",
      "epoch 766/1000   error=0.001251\n",
      "epoch 767/1000   error=0.001247\n",
      "epoch 768/1000   error=0.001244\n",
      "epoch 769/1000   error=0.001240\n",
      "epoch 770/1000   error=0.001236\n",
      "epoch 771/1000   error=0.001232\n",
      "epoch 772/1000   error=0.001229\n",
      "epoch 773/1000   error=0.001225\n",
      "epoch 774/1000   error=0.001221\n",
      "epoch 775/1000   error=0.001218\n",
      "epoch 776/1000   error=0.001214\n",
      "epoch 777/1000   error=0.001211\n",
      "epoch 778/1000   error=0.001207\n",
      "epoch 779/1000   error=0.001203\n",
      "epoch 780/1000   error=0.001200\n",
      "epoch 781/1000   error=0.001196\n",
      "epoch 782/1000   error=0.001193\n",
      "epoch 783/1000   error=0.001189\n",
      "epoch 784/1000   error=0.001186\n",
      "epoch 785/1000   error=0.001182\n",
      "epoch 786/1000   error=0.001179\n",
      "epoch 787/1000   error=0.001176\n",
      "epoch 788/1000   error=0.001172\n",
      "epoch 789/1000   error=0.001169\n",
      "epoch 790/1000   error=0.001165\n",
      "epoch 791/1000   error=0.001162\n",
      "epoch 792/1000   error=0.001159\n",
      "epoch 793/1000   error=0.001155\n",
      "epoch 794/1000   error=0.001152\n",
      "epoch 795/1000   error=0.001149\n",
      "epoch 796/1000   error=0.001146\n",
      "epoch 797/1000   error=0.001142\n",
      "epoch 798/1000   error=0.001139\n",
      "epoch 799/1000   error=0.001136\n",
      "epoch 800/1000   error=0.001133\n",
      "epoch 801/1000   error=0.001130\n",
      "epoch 802/1000   error=0.001126\n",
      "epoch 803/1000   error=0.001123\n",
      "epoch 804/1000   error=0.001120\n",
      "epoch 805/1000   error=0.001117\n",
      "epoch 806/1000   error=0.001114\n",
      "epoch 807/1000   error=0.001111\n",
      "epoch 808/1000   error=0.001108\n",
      "epoch 809/1000   error=0.001105\n",
      "epoch 810/1000   error=0.001102\n",
      "epoch 811/1000   error=0.001099\n",
      "epoch 812/1000   error=0.001096\n",
      "epoch 813/1000   error=0.001093\n",
      "epoch 814/1000   error=0.001090\n",
      "epoch 815/1000   error=0.001087\n",
      "epoch 816/1000   error=0.001084\n",
      "epoch 817/1000   error=0.001081\n",
      "epoch 818/1000   error=0.001078\n",
      "epoch 819/1000   error=0.001075\n",
      "epoch 820/1000   error=0.001072\n",
      "epoch 821/1000   error=0.001069\n",
      "epoch 822/1000   error=0.001066\n",
      "epoch 823/1000   error=0.001063\n",
      "epoch 824/1000   error=0.001060\n",
      "epoch 825/1000   error=0.001058\n",
      "epoch 826/1000   error=0.001055\n",
      "epoch 827/1000   error=0.001052\n",
      "epoch 828/1000   error=0.001049\n",
      "epoch 829/1000   error=0.001046\n",
      "epoch 830/1000   error=0.001044\n",
      "epoch 831/1000   error=0.001041\n",
      "epoch 832/1000   error=0.001038\n",
      "epoch 833/1000   error=0.001035\n",
      "epoch 834/1000   error=0.001033\n",
      "epoch 835/1000   error=0.001030\n",
      "epoch 836/1000   error=0.001027\n",
      "epoch 837/1000   error=0.001025\n",
      "epoch 838/1000   error=0.001022\n",
      "epoch 839/1000   error=0.001019\n",
      "epoch 840/1000   error=0.001017\n",
      "epoch 841/1000   error=0.001014\n",
      "epoch 842/1000   error=0.001012\n",
      "epoch 843/1000   error=0.001009\n",
      "epoch 844/1000   error=0.001006\n",
      "epoch 845/1000   error=0.001004\n",
      "epoch 846/1000   error=0.001001\n",
      "epoch 847/1000   error=0.000999\n",
      "epoch 848/1000   error=0.000996\n",
      "epoch 849/1000   error=0.000994\n",
      "epoch 850/1000   error=0.000991\n",
      "epoch 851/1000   error=0.000989\n",
      "epoch 852/1000   error=0.000986\n",
      "epoch 853/1000   error=0.000984\n",
      "epoch 854/1000   error=0.000981\n",
      "epoch 855/1000   error=0.000979\n",
      "epoch 856/1000   error=0.000976\n",
      "epoch 857/1000   error=0.000974\n",
      "epoch 858/1000   error=0.000971\n",
      "epoch 859/1000   error=0.000969\n",
      "epoch 860/1000   error=0.000967\n",
      "epoch 861/1000   error=0.000964\n",
      "epoch 862/1000   error=0.000962\n",
      "epoch 863/1000   error=0.000960\n",
      "epoch 864/1000   error=0.000957\n",
      "epoch 865/1000   error=0.000955\n",
      "epoch 866/1000   error=0.000953\n",
      "epoch 867/1000   error=0.000950\n",
      "epoch 868/1000   error=0.000948\n",
      "epoch 869/1000   error=0.000946\n",
      "epoch 870/1000   error=0.000943\n",
      "epoch 871/1000   error=0.000941\n",
      "epoch 872/1000   error=0.000939\n",
      "epoch 873/1000   error=0.000936\n",
      "epoch 874/1000   error=0.000934\n",
      "epoch 875/1000   error=0.000932\n",
      "epoch 876/1000   error=0.000930\n",
      "epoch 877/1000   error=0.000928\n",
      "epoch 878/1000   error=0.000925\n",
      "epoch 879/1000   error=0.000923\n",
      "epoch 880/1000   error=0.000921\n",
      "epoch 881/1000   error=0.000919\n",
      "epoch 882/1000   error=0.000917\n",
      "epoch 883/1000   error=0.000914\n",
      "epoch 884/1000   error=0.000912\n",
      "epoch 885/1000   error=0.000910\n",
      "epoch 886/1000   error=0.000908\n",
      "epoch 887/1000   error=0.000906\n",
      "epoch 888/1000   error=0.000904\n",
      "epoch 889/1000   error=0.000902\n",
      "epoch 890/1000   error=0.000900\n",
      "epoch 891/1000   error=0.000898\n",
      "epoch 892/1000   error=0.000895\n",
      "epoch 893/1000   error=0.000893\n",
      "epoch 894/1000   error=0.000891\n",
      "epoch 895/1000   error=0.000889\n",
      "epoch 896/1000   error=0.000887\n",
      "epoch 897/1000   error=0.000885\n",
      "epoch 898/1000   error=0.000883\n",
      "epoch 899/1000   error=0.000881\n",
      "epoch 900/1000   error=0.000879\n",
      "epoch 901/1000   error=0.000877\n",
      "epoch 902/1000   error=0.000875\n",
      "epoch 903/1000   error=0.000873\n",
      "epoch 904/1000   error=0.000871\n",
      "epoch 905/1000   error=0.000869\n",
      "epoch 906/1000   error=0.000867\n",
      "epoch 907/1000   error=0.000865\n",
      "epoch 908/1000   error=0.000864\n",
      "epoch 909/1000   error=0.000862\n",
      "epoch 910/1000   error=0.000860\n",
      "epoch 911/1000   error=0.000858\n",
      "epoch 912/1000   error=0.000856\n",
      "epoch 913/1000   error=0.000854\n",
      "epoch 914/1000   error=0.000852\n",
      "epoch 915/1000   error=0.000850\n",
      "epoch 916/1000   error=0.000848\n",
      "epoch 917/1000   error=0.000847\n",
      "epoch 918/1000   error=0.000845\n",
      "epoch 919/1000   error=0.000843\n",
      "epoch 920/1000   error=0.000841\n",
      "epoch 921/1000   error=0.000839\n",
      "epoch 922/1000   error=0.000837\n",
      "epoch 923/1000   error=0.000836\n",
      "epoch 924/1000   error=0.000834\n",
      "epoch 925/1000   error=0.000832\n",
      "epoch 926/1000   error=0.000830\n",
      "epoch 927/1000   error=0.000828\n",
      "epoch 928/1000   error=0.000827\n",
      "epoch 929/1000   error=0.000825\n",
      "epoch 930/1000   error=0.000823\n",
      "epoch 931/1000   error=0.000821\n",
      "epoch 932/1000   error=0.000820\n",
      "epoch 933/1000   error=0.000818\n",
      "epoch 934/1000   error=0.000816\n",
      "epoch 935/1000   error=0.000814\n",
      "epoch 936/1000   error=0.000813\n",
      "epoch 937/1000   error=0.000811\n",
      "epoch 938/1000   error=0.000809\n",
      "epoch 939/1000   error=0.000807\n",
      "epoch 940/1000   error=0.000806\n",
      "epoch 941/1000   error=0.000804\n",
      "epoch 942/1000   error=0.000802\n",
      "epoch 943/1000   error=0.000801\n",
      "epoch 944/1000   error=0.000799\n",
      "epoch 945/1000   error=0.000797\n",
      "epoch 946/1000   error=0.000796\n",
      "epoch 947/1000   error=0.000794\n",
      "epoch 948/1000   error=0.000792\n",
      "epoch 949/1000   error=0.000791\n",
      "epoch 950/1000   error=0.000789\n",
      "epoch 951/1000   error=0.000788\n",
      "epoch 952/1000   error=0.000786\n",
      "epoch 953/1000   error=0.000784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 954/1000   error=0.000783\n",
      "epoch 955/1000   error=0.000781\n",
      "epoch 956/1000   error=0.000780\n",
      "epoch 957/1000   error=0.000778\n",
      "epoch 958/1000   error=0.000776\n",
      "epoch 959/1000   error=0.000775\n",
      "epoch 960/1000   error=0.000773\n",
      "epoch 961/1000   error=0.000772\n",
      "epoch 962/1000   error=0.000770\n",
      "epoch 963/1000   error=0.000769\n",
      "epoch 964/1000   error=0.000767\n",
      "epoch 965/1000   error=0.000766\n",
      "epoch 966/1000   error=0.000764\n",
      "epoch 967/1000   error=0.000763\n",
      "epoch 968/1000   error=0.000761\n",
      "epoch 969/1000   error=0.000759\n",
      "epoch 970/1000   error=0.000758\n",
      "epoch 971/1000   error=0.000756\n",
      "epoch 972/1000   error=0.000755\n",
      "epoch 973/1000   error=0.000754\n",
      "epoch 974/1000   error=0.000752\n",
      "epoch 975/1000   error=0.000751\n",
      "epoch 976/1000   error=0.000749\n",
      "epoch 977/1000   error=0.000748\n",
      "epoch 978/1000   error=0.000746\n",
      "epoch 979/1000   error=0.000745\n",
      "epoch 980/1000   error=0.000743\n",
      "epoch 981/1000   error=0.000742\n",
      "epoch 982/1000   error=0.000740\n",
      "epoch 983/1000   error=0.000739\n",
      "epoch 984/1000   error=0.000737\n",
      "epoch 985/1000   error=0.000736\n",
      "epoch 986/1000   error=0.000735\n",
      "epoch 987/1000   error=0.000733\n",
      "epoch 988/1000   error=0.000732\n",
      "epoch 989/1000   error=0.000730\n",
      "epoch 990/1000   error=0.000729\n",
      "epoch 991/1000   error=0.000728\n",
      "epoch 992/1000   error=0.000726\n",
      "epoch 993/1000   error=0.000725\n",
      "epoch 994/1000   error=0.000723\n",
      "epoch 995/1000   error=0.000722\n",
      "epoch 996/1000   error=0.000721\n",
      "epoch 997/1000   error=0.000719\n",
      "epoch 998/1000   error=0.000718\n",
      "epoch 999/1000   error=0.000717\n",
      "epoch 1000/1000   error=0.000715\n",
      "predicted =  [array([[[0.60507562, 0.66384844],\n",
      "        [0.2976441 , 0.72487527],\n",
      "        [0.81136151, 0.74092919],\n",
      "        [0.29234143, 0.28330749]],\n",
      "\n",
      "       [[0.77744309, 0.77339204],\n",
      "        [0.15656436, 0.2894298 ],\n",
      "        [0.37307734, 0.14813683],\n",
      "        [0.09209443, 0.51723669]],\n",
      "\n",
      "       [[0.69459942, 0.7319246 ],\n",
      "        [0.39671856, 0.4188934 ],\n",
      "        [0.90601359, 0.03516927],\n",
      "        [0.07438397, 0.06231427]],\n",
      "\n",
      "       [[0.64496879, 0.2293445 ],\n",
      "        [0.45374355, 0.70897586],\n",
      "        [0.5198563 , 0.55754499],\n",
      "        [0.52760025, 0.24649759]]])]\n",
      "expected =  [array([[[0.61356309, 0.70001481],\n",
      "        [0.2968138 , 0.72660313],\n",
      "        [0.81380823, 0.80136373],\n",
      "        [0.29399745, 0.28806546]],\n",
      "\n",
      "       [[0.77803525, 0.81616926],\n",
      "        [0.14658051, 0.27649694],\n",
      "        [0.37704518, 0.17570618],\n",
      "        [0.09943719, 0.53526787]],\n",
      "\n",
      "       [[0.65295593, 0.66932863],\n",
      "        [0.37712169, 0.44631944],\n",
      "        [0.9656428 , 0.01064791],\n",
      "        [0.07190077, 0.06235705]],\n",
      "\n",
      "       [[0.64914174, 0.20948043],\n",
      "        [0.46409809, 0.66125035],\n",
      "        [0.53063786, 0.53430189],\n",
      "        [0.53807485, 0.24303531]]])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from network import Network\n",
    "from conv_layer import ConvLayer\n",
    "from activation_layer import ActivationLayer\n",
    "from activations import tanh, tanh_prime\n",
    "from losses import mse, mse_prime\n",
    "\n",
    "# training data\n",
    "x_train = [np.random.rand(10,10,1)]\n",
    "y_train = [np.random.rand(4,4,2)]\n",
    "\n",
    "# network\n",
    "net = Network()\n",
    "net.add(ConvLayer((10,10,1), (3,3), 1))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(ConvLayer((8,8,1), (3,3), 1))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(ConvLayer((6,6,1), (3,3), 2))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "\n",
    "# train\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=1000, learning_rate=0.3)\n",
    "\n",
    "# test\n",
    "out = net.predict(x_train)\n",
    "print(\"predicted = \", out)\n",
    "print(\"expected = \", y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2f455a",
   "metadata": {},
   "source": [
    "# example_mnist_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3225fa1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wm/14w2c7751klgtnkgn1yj14m00000gn/T/ipykernel_11901/2453768631.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_prime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \"\"\"\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from network import Network\n",
    "from fc_layer import FCLayer\n",
    "from conv_layer import ConvLayer\n",
    "from flatten_layer import FlattenLayer\n",
    "from activation_layer import ActivationLayer\n",
    "from activations import tanh, tanh_prime\n",
    "from losses import mse, mse_prime\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load MNIST from server\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# training data : 60000 samples\n",
    "# reshape and normalize input data \n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "# encode output which is a number in range [0,9] into a vector of size 10\n",
    "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "# same for test data : 10000 samples\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# Network\n",
    "net = Network()\n",
    "net.add(ConvLayer((28, 28, 1), (3, 3), 1))  # input_shape=(28, 28, 1)   ;   output_shape=(26, 26, 1) \n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FlattenLayer())                     # input_shape=(26, 26, 1)   ;   output_shape=(1, 26*26*1)\n",
    "net.add(FCLayer(26*26*1, 100))              # input_shape=(1, 26*26*1)  ;   output_shape=(1, 100)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(100, 10))                   # input_shape=(1, 100)      ;   output_shape=(1, 10)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "\n",
    "# train on 1000 samples\n",
    "# as we didn't implemented mini-batch GD, training will be pretty slow if we update at each iteration on 60000 samples...\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train[0:1000], y_train[0:1000], epochs=100, learning_rate=0.1)\n",
    "\n",
    "# test on 3 samples\n",
    "out = net.predict(x_test[0:3])\n",
    "print(\"\\n\")\n",
    "print(\"predicted values : \")\n",
    "print(out, end=\"\\n\")\n",
    "print(\"true values : \")\n",
    "print(y_test[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1393149",
   "metadata": {},
   "source": [
    "# example_mnist_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3e5f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from network import Network\n",
    "from fc_layer import FCLayer\n",
    "from activation_layer import ActivationLayer\n",
    "from activations import tanh, tanh_prime\n",
    "from losses import mse, mse_prime\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load MNIST from server\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# training data : 60000 samples\n",
    "# reshape and normalize input data\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "# encode output which is a number in range [0,9] into a vector of size 10\n",
    "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "# same for test data : 10000 samples\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# Network\n",
    "net = Network()\n",
    "net.add(FCLayer(28*28, 100))                # input_shape=(1, 28*28)    ;   output_shape=(1, 100)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(100, 50))                   # input_shape=(1, 100)      ;   output_shape=(1, 50)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(50, 10))                    # input_shape=(1, 50)       ;   output_shape=(1, 10)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "\n",
    "# train on 1000 samples\n",
    "# as we didn't implemented mini-batch GD, training will be pretty slow if we update at each iteration on 60000 samples...\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train[0:1000], y_train[0:1000], epochs=35, learning_rate=0.1)\n",
    "\n",
    "# test on 3 samples\n",
    "out = net.predict(x_test[0:3])\n",
    "print(\"\\n\")\n",
    "print(\"predicted values : \")\n",
    "print(out, end=\"\\n\")\n",
    "print(\"true values : \")\n",
    "print(y_test[0:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baa38e6",
   "metadata": {},
   "source": [
    "# example_xor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f34dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from network import Network\n",
    "from fc_layer import FCLayer\n",
    "from activation_layer import ActivationLayer\n",
    "from activations import tanh, tanh_prime\n",
    "from losses import mse, mse_prime\n",
    "\n",
    "# training data\n",
    "x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\n",
    "y_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n",
    "\n",
    "# network\n",
    "net = Network()\n",
    "net.add(FCLayer(2, 3))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(3, 1))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "\n",
    "# train\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=1000, learning_rate=0.1)\n",
    "\n",
    "# test\n",
    "out = net.predict(x_train)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fbaf2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
